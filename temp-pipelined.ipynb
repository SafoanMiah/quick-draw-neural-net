{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "from tqdm import tqdm   \n",
    "from torch.utils.data import DataLoader, ConcatDataset\n",
    "\n",
    "from utils.model_train import train\n",
    "from utils.processing import load_data, process_data, augment_data, to_tensors, split_batch, incremental_save, partition_datasets\n",
    "from utils.quickdraw_cnn import QuickDrawCNN_V1, QuickDrawCNN_V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dir = \"data/numpy_bitmap/\"\n",
    "datasets_dir = \"data/datasets\"\n",
    "categories = os.listdir(image_dir)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "labels_map = {label: i for i, label in enumerate(set(categories))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline(image_dir: str, categories: list, device: torch.device, labels_map: dict):\n",
    "    \n",
    "    '''\n",
    "    A pipeline function to run the entire process of loading, processing, augmenting, and splitting the data onto each category at a time.\n",
    "    '''\n",
    "    \n",
    "    train_datasets, test_datasets, val_datasets = [], [], []\n",
    "\n",
    "    for cat in tqdm(categories, desc=\"Processing categories\"):\n",
    "        \n",
    "        try:\n",
    "            # load, process, augment, and split the data\n",
    "            features, label = load_data(image_dir, cat, file_standardize=False)\n",
    "            features, label = process_data(features, label)\n",
    "            mask = np.random.rand(len(features)) <= 0.4 # drop 60% of the data\n",
    "            features = features[mask]\n",
    "            # features, label = augment_data(features, label, rot=0, h_flip=False, v_flip=False)\n",
    "            features, labels = to_tensors(features, label, labels_map, device=device)\n",
    "            \n",
    "            # split the data into train, test, and validation sets\n",
    "            train_loader, test_loader, val_loader = split_batch(features, labels, batch_size=32)\n",
    "            train_datasets.append(train_loader.dataset)\n",
    "            test_datasets.append(test_loader.dataset)\n",
    "            val_datasets.append(val_loader.dataset)\n",
    "        except RuntimeError as e:\n",
    "            print(f\"Error on {cat}: {e}\")\n",
    "        \n",
    "    return train_datasets, test_datasets, val_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_files = 15 \n",
    "\n",
    "train_file_paths = [f\"{datasets_dir}/train/train_datasets_{i}.pkl\" for i in range(num_files)]\n",
    "test_file_paths = [f\"{datasets_dir}/test/test_datasets_{i}.pkl\" for i in range(num_files)]\n",
    "val_file_paths = [f\"{datasets_dir}/val/val_datasets_{i}.pkl\" for i in range(num_files)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tqdm(range(161, len(categories), len(categories) // num_files), desc=\"Processing categorical split\"):\n",
    "    print(f\"Processing categories {i} to {min(i + len(categories) // num_files, len(categories))}...\")\n",
    "    \n",
    "    train_buffers = [[] for _ in range(num_files)]\n",
    "    test_buffers = [[] for _ in range(num_files)]\n",
    "    val_buffers = [[] for _ in range(num_files)]\n",
    "        \n",
    "    try:\n",
    "        train_datasets, test_datasets, val_datasets = pipeline(image_dir, categories[i:i + len(categories) // num_files], device, labels_map)\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        # Distribute datasets into buffers\n",
    "        for idx, (train_data, test_data, val_data) in enumerate(tqdm(zip(train_datasets, test_datasets, val_datasets), desc=\"Distributing datasets\", total=len(train_datasets))):\n",
    "            file_idx = idx % num_files\n",
    "            train_buffers[file_idx].append(train_data)\n",
    "            test_buffers[file_idx].append(test_data)\n",
    "            val_buffers[file_idx].append(val_data)\n",
    "\n",
    "        # Save the buffers to their respective files\n",
    "        for file_idx in tqdm(range(num_files), desc=\"Saving buffers\"):\n",
    "            with open(train_file_paths[file_idx], \"wb\") as f:\n",
    "                pickle.dump(train_buffers[file_idx], f)\n",
    "            with open(test_file_paths[file_idx], \"wb\") as f:\n",
    "                pickle.dump(test_buffers[file_idx], f)\n",
    "            with open(val_file_paths[file_idx], \"wb\") as f:\n",
    "                pickle.dump(val_buffers[file_idx], f)\n",
    "\n",
    "        # Clear buffers to free up memory\n",
    "        del train_datasets, test_datasets, val_datasets\n",
    "        del train_data, test_data, val_data\n",
    "        del train_buffers, test_buffers, val_buffers\n",
    "        \n",
    "        \n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    except RuntimeError as e:\n",
    "        print(f\"Error on {i}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data back\n",
    "train_datasets = []\n",
    "test_datasets = []\n",
    "\n",
    "print(os.listdir(f\"{datasets_dir}/train\"))\n",
    "\n",
    "for file in tqdm(os.listdir(f\"{datasets_dir}/train\"), desc=\"Loading train datasets\"):\n",
    "    with open(f\"{datasets_dir}/train/{file}\", \"rb\") as f:\n",
    "        train_datasets.extend(pickle.load(f))\n",
    "for file in tqdm(os.listdir(f\"{datasets_dir}/test\"), desc=\"Loading test datasets\"):\n",
    "    with open(f\"{datasets_dir}/test/{file}\", \"rb\") as f:\n",
    "        test_datasets.extend(pickle.load(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concat dataloaders\n",
    "train_loader = DataLoader(ConcatDataset(train_datasets), batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(ConcatDataset(test_datasets), batch_size=64, shuffle=False)\n",
    "val_loader = DataLoader(ConcatDataset(val_datasets), batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = QuickDrawCNN_V2().to(device)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    train_loss, val_loss, train_acc, val_acc = train(model=model,\n",
    "                                                     train_loader=train_loader,\n",
    "                                                     val_loader=val_loader,\n",
    "                                                     epochs=10,\n",
    "                                                     criterion=criterion,\n",
    "                                                     optimizer=optimizer,\n",
    "                                                     device=device)\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Training interrupted manually. Saving current model state...\")\n",
    "    torch.save(model.state_dict(), \"model_state.pth\")\n",
    "    print(\"Model state saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path: saves/vars.0.pth\n",
      "Saved to saves/model.0.pth.\n"
     ]
    }
   ],
   "source": [
    "parent_path = 'saves'\n",
    "model_path = f'{parent_path}/model'\n",
    "var_path = f'{parent_path}/vars'\n",
    "\n",
    "varaibles_saved = incremental_save(var_path)\n",
    "with open(varaibles_saved, \"wb\") as f:\n",
    "    pickle.dump(labels_map, f)\n",
    "    pickle.dump(train_loss, f)\n",
    "    pickle.dump(val_loss, f)\n",
    "    pickle.dump(train_acc, f)\n",
    "    pickle.dump(val_acc, f)\n",
    "\n",
    "model_saved = incremental_save(model_path, data=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
